rm(noun)
class(nouns)
length(nouns)
nouns[[1]]
head(nouns)
unlist(nouns)
length(unlist(nouns))
table(unlist(nouns))
wordcount <- table(unlist(nouns))
table(c('하나', '둘', '셋'))
wordcount <- table(unlist(nouns))
class(wordcount)
length(wordcount)
sort(wordcount, decreasing = T)
library(dplyr)
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
head(df_word)
df_word <- rename(df_word, word = Var1,
freq = Freq)
head(df_word)
df_word$word <- trim(df_word$word)
head(df_word)
### 한글자 이상의 단어만 추출
df_word <- df_word %>%
filter(nchar(word) > 1)
### 단어 빈도표 만들기 (Top 20)
# top20 <- df_word[order(df_word$freq, decreasing = T), ]
top20 <- df_word[order(-df_word$freq), ]
top20
### 단어 빈도표 만들기 (Top 20)
# top20 <- df_word[order(df_word$freq, decreasing = T), ]
top20 <- df_word[order(-df_word$freq), ][1:20]
### 단어 빈도표 만들기 (Top 20)
# top20 <- df_word[order(df_word$freq, decreasing = T), ]
top20 <- df_word[order(-df_word$freq), ][1:20, ]
top20
### 단어 빈도표를 그래프로 그리기
library(ggplot2)
ggplot(top20, aes(x = reorder(word, freq), y = freq)) +
geom_col() +
coord_flip()
ggplot(top20, aes(x = freq, y = reorder(word, freq))) +
geom_col()
ggplot(top20, aes(x = freq, y = reorder(word, freq))) +
geom_col() +
geom_text(aes(label = freq), col = 'red')
ggplot(top20, aes(x = freq, y = reorder(word, freq))) +
geom_col() +
geom_text(aes(label = freq), col = 'red', nudge_x = 2)
## 워드 클라우드
# 1) 비정형 텍스트 데이터 확보
# 2) 패키지 설치 및 로드 (KoNLP, wordcloud)
install.packages('wordcloud')
library(wordcloud)
display.brewer.all()
# 2-2) 난수 생성의 결과 일치시키기
set.seed(1234)
round(runif(6, min = 1, max = 45))
round(runif(6, min = 1, max = 45))
# 3) 확보된 txt 데이터 읽어오기 (벡터 형태)
# 4) 명사 추출
# 5) 필요없는 데이터 삭제 (특수문자, zz, ㅋㅋ, ㅎㅎ, ...)
# 6) 워드 클라우드 생성 (dataframe)
# 7) wordcloud함수를 이용해서 워드클라우드 생성
wordcloud(words = df_word$word, freq = df_word$freq, # 단어와 빈도수 설정
min.freq = 2, max.words = 200, # 최소, 최대한의 단어 빈도수
random.order = F, # 최다 빈도수의 단어가 중앙에 배치
rot.per = 0.1, scale = c(4, 0.3), # 회전 단어 비율 및 단어 크기 범위
colors = pal)
# 2-1) 워드클라우드에 쓸 파레트 변수 설정
pal <- brewer.pal(9, 'blues')[5:9]
display.brewer.all()
# 2-1) 워드클라우드에 쓸 파레트 변수 설정
pal <- brewer.pal(9, 'Blues')[5:9]
# 3) 확보된 txt 데이터 읽어오기 (벡터 형태)
# 4) 명사 추출
# 5) 필요없는 데이터 삭제 (특수문자, zz, ㅋㅋ, ㅎㅎ, ...)
# 6) 워드 클라우드 생성 (dataframe)
# 7) wordcloud함수를 이용해서 워드클라우드 생성
wordcloud(words = df_word$word, freq = df_word$freq, # 단어와 빈도수 설정
min.freq = 2, max.words = 200, # 최소, 최대한의 단어 빈도수
random.order = F, # 최다 빈도수의 단어가 중앙에 배치
rot.per = 0.1, scale = c(4, 0.3), # 회전 단어 비율 및 단어 크기 범위
colors = pal)
##### 2. 국정원 트위터 데이터 #####
rm(list = ls(all.names = T))
library(KoNLP)
library(wordcloud)
library(stringr)
library(dplyr)
library(ggplot2)
twitter <- read.csv('inData/twitter.csv', header = T,
stringsAsFactors = F, fileEncoding = 'utf-8')
head(twitter)
View(twitter)
twitter$내용
twitter <- rename(twitter, no = 번호, id = 계정이름, date = 작성일, tw = 내용)
View(twitter)
# 필요없는 문자 삭제
twitter$tw <- str_replace_all(twitter$tw, '\\W', ' ')
twitter$tw <- str_replace_all(twitter$tw, '[ㄱ-ㅎ]+', ' ')
head(twitter$tw)
twitter$tw <- trim(twitter$tw)
# 명사 추출
class(twitter$tw)
nouns <- extractNoun(twitter$tw)
# 워드 카운트
wordcount <- table(unlist(nouns))
head(sort(wordcount, decreasing = T))
length(unlist(nouns))
length(wordcount)
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word <- rename(df_word, word = Var1, freq = Freq)
str(df_word)
# 단어 중 두글자 이상만 분석
df_word <- filter(df_word, nchar(word) > 1)
# 최빈 단어 top 20 그래프
df_word[order(-df_word$freq), ][1:20, ]
# 최빈 단어 top 20 그래프
top20 <- df_word[order(-df_word$freq), ][1:20, ]
ggplot(top20, aes(x = freq, y = reorder(word, freq))) +
geom_bar(stat = 'identity') +
geom_text(aes(label = freq), hjust = 1, col = 'orange')
ggplot(top20, aes(x = freq, y = reorder(word, freq))) +
geom_bar(stat = 'identity') +
geom_text(aes(label = freq), hjust = 2, col = 'orange')
ggplot(top20, aes(x = freq, y = reorder(word, freq))) +
geom_bar(stat = 'identity') +
geom_text(aes(label = freq), hjust = 1.5, col = 'orange')
# 워드 클라우드
set.seed(1234)
pal <- brewer.pal(8, "Dark2")
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(3, 0, 3), colors = pal)
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(3, 0.2), colors = pal)
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(2, 0.2), colors = pal)
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(6, 0.2), colors = pal)
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(20, 0.2), colors = pal)
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(15, 0.2), colors = pal)
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(10, 0.2), colors = pal)
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(6, 0.3), colors = pal)
# 게시글의 횟수가 150회 이상인 데이터에 대해 시각화
table(twitter$id)
# 게시글의 횟수가 150회 이상인 데이터에 대해 시각화
idCount <- as.data.frame(table(twitter$id))
str(idCount)
idCount <- rename(idCount, word = Var1, freq = Freq)
str(idCount)
head(idCount)
idCount <- rename(idCount, id = Var1, freq = Freq)
idCount <- rename(idCount, id = word, freq = Freq)
idCount <- rename(idCount, id = word)
idCount %>%
group_by(id) %>%
summarise(cnt = n())
twitter1 <- left_join(twitter, idCount, by = 'id')
head(twitter1)
twitter1 <- subset(twitter, count > 150)
rm(twitter1)
twitter <- left_join(twitter, idCount, by = 'id')
twitter1 <- subset(twitter, count > 150)
idCount <- rename(idCount, count = freq)
twitter1 <- subset(twitter, count>150)
twitter <- left_join(twitter, idCount, by = 'id')
twitter1 <- subset(twitter, count>150)
head(twitter1)
rm(idCount)
# 게시글의 횟수가 150회 이상인 데이터에 대해 시각화
idCount <- as.data.frame(table(twitter$id))
idCount <- rename(idCount, id = word)
idCount <- rename(idCount, count = freq)
idCount <- rename(idCount, id = Var1)
idCount <- rename(idCount, count = Freq)
twitter <- left_join(twitter, idCount, by = 'id')
twitter1 <- subset(twitter, count>150)
head(twitter1)
rm(idCount)
rm(twitter1)
# 게시글의 횟수가 150회 이상인 데이터에 대해 시각화
idCount <- as.data.frame(table(twitter$id))
idCount <- rename(idCount, id = Var1)
idCount <- rename(idCount, count = Freq)
str(idCount)
twitter <- left_join(twitter, idCount, by = 'id')
twitter1 <- subset(twitter, count>150)
head(twitter1)
##### 2. 국정원 트위터 데이터 #####
rm(list = ls(all.names = T))
library(KoNLP)
library(wordcloud)
library(stringr)
library(dplyr)
library(ggplot2)
twitter <- read.csv('inData/twitter.csv', header = T,
stringsAsFactors = F, fileEncoding = 'utf-8')
twitter <- rename(twitter, no = 번호, id = 계정이름, date = 작성일, tw = 내용)
# 필요없는 문자 삭제
twitter$tw <- str_replace_all(twitter$tw, '\\W', ' ')
twitter$tw <- str_replace_all(twitter$tw, '[ㄱ-ㅎ]+', ' ')
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
# 워드 카운트 생성
wordcount <- table(unlist(nouns))
nouns <- extractNoun(twitter$tw)
# 워드 카운트 생성
wordcount <- table(unlist(nouns))
# 필요없는 문자 삭제
# 명사 추출
nouns <- extractNoun(twitter$tw)
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word <- rename(df_word, word = Var1, freq = Freq)
# 워드 클라우드
set.seed(1234)
# 게시글의 횟수가 150회 이상인 데이터에 대해 시각화
idCount <- as.data.frame(table(twitter$id))
idCount <- rename(idCount, id = Var1)
idCount <- rename(idCount, count = Freq)
twitter <- left_join(twitter, idCount, by = 'id')
twitter1 <- subset(twitter, count>150)
head(twitter1)
# 필요없는 문자 삭제
# 명사 추출
nouns <- extractNoun(twitter$tw)
# 워드 카운트
wordcount <- table(unlist(nouns))
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word <- rename(df_word, word = Var1, freq = Freq)
head(df_word)
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(2, 0.2), colors = pal)
pal <- brewer.pal(8, "Dark2")
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(2, 0.2), colors = pal)
df_word <- filter(df_word, nchar(word) > 1)
wordcloud(words = df_word$word, freq = df_word$freq,
min.freq = 5, max.words = 200, random.order = F,
rot.per = 0.1, scale = c(2, 0.2), colors = pal)
head(df_word)
################################
##### 13장. 웹 데이터 수집 #####
################################
rm(ls(all.names = T))
################################################
##### 13장. 웹 데이터 수집 (정적 웹크롤링) #####
################################################
# 패키지 설치 및 로드
install.packages("rvest")
library(rvest)
################################################
##### 13장. 웹 데이터 수집 (정적 웹크롤링) #####
################################################
# 패키지 설치 및 로드
installed.packages()
url <- 'https://movie.naver.com/movie/point/af/list.nhn'
html <- read_html(url, encoding = 'utf-8')
head(html)
html
# 영화제목 : .movie(.color_b)
nodes <- html_nodes(html, '.movie')
as.character(nodes)
title <- html_text(nodes)
head(title)
# 해당 영화 안내 페이지
movie_info <- html_attr(nodes, 'href')
movie_info <- paste0(url, movie_info)
head(movie_info)
# 평점 : < .list_netizen_score em >
nodes <- html_nodes(html, '.list_netizen_score em')
nodes
movie_point <- html_text(nodes)
movie_point
class(movie_point)
# 영화 리뷰 : td.title
nodes <- html_nodes(html, 'td.title')
text <- html_text(nodes)
text
? gsub
text <- gsub('\t', text)
text <- gsub('\t',' ', text)
text
text <- gsub('\n', ' ', text)
text
text
text
text <- gsub('\t','', text)
text <- gsub('\n', '', text)
text
source("D:/Gray_Bigdata/src/06_R/13장_웹데이터수집.R", encoding = 'UTF-8', echo=TRUE)
text <- gsub(' ', '', txt)
text <- gsub(' ', '', text)
text
strsplit(text, '중[0-9]{1,2}') # 0~9까지가 1개 혹은 두개
unlist( strsplit(text, '중[0-9]{1,2}') ) # 0~9까지가 1개 혹은 두개
unlist( strsplit(text, '중[0-9]{1,2}') )[seq(2, 20, 2)] # 0~9까지가 1개 혹은 두개
review <- unlist( strsplit(text, '중[0-9]{1,2}') )[seq(2, 20, 2)] # 0~9까지가 1개 혹은 두개
gsub('신고', '', review)
review <- gsub('신고', '', review)
review
rm(text)
text <- html_text(nodes)
text
text <- gsub('\t','', text)
text <- gsub('\n', '', text)
text
review <- unlist( strsplit(text, '중[0-9]{1,2}') )[seq(2, 20, 2)] # 0~9까지가 1개 혹은 두개
review <- gsub('신고', '', review)
review
# 데이터 프레임으로 만들기
df <- data.frame(title, movie_info, movie_point, review)
df
View(df)
page <- cbind(title, movie_info)
page
page <- cbind(page, movie_point)
page <- cbind(page, review)
page
write.csv(df, file = 'outData/movie_review.csv')
View(page)
### 여러 페이지 정적 웹 크롤링 (영화리뷰 1~100페이지까지) ###
home <- 'https://movie.naver.com/movie/point/af/list.nhn'
site <- 'https://movie.naver.com/movie/point/af/list.nhn?&page='
movie.review <- NULL
for (i in 1:100) {
url <- paste0(site, i)
print(url)
}
for (i in 1:100) {
url <- paste0(site, i)
print(url)
html <- read_html(url, encoding = 'utf-8')
# 영화제목 : < .movie(.color_b) >
nodes <- html_nodes(html, '.movie')
title <- html_text(nodes)
# 해당 영화 안내 페이지
movie_info <- html_attr(nodes, 'href')
movie_info <- paste0(home, movie_info) # for문을 돌릴 때는 변수 확인 필수
# 평점 : < .list_netizen_score em >
nodes <- html_nodes(html, '.list_netizen_score em')
movie_point <- html_text(nodes)
# 영화 리뷰 : td.title
nodes <- html_nodes(html, 'td.title')
text <- html_text(nodes)
text <- gsub('\t','', text)
text <- gsub('\n', '', text)
review <- unlist( strsplit(text, '중[0-9]{1,2}') )[seq(2, 20, 2)] # 0~9까지가 1개 혹은 두개
review <- gsub('신고', '', review)
# 데이터 프레임으로 만들기
df <- data.frame(title, movie_info, movie_point, review) # 한페이지에 4열 10행이 생성된다.
movie.review <- rbind(movie.review, df)
}
View(movie.review)
head(movie.review)
table(movie.review$title)
write.csv(movie.review, file = 'outData/movie_review')
write.csv(movie.review, file = 'outData/movie_review.csv')
write.csv(movie.review, file = 'outData/naver_movie.csv')
library(dplyr)
movie.review %>%
group_by(movie_point) %>%
summarise(cnt = n())
movie.review %>%
group_by(title) %>%
summarise(cnt = n())
movie.review %>%
group_by(title) %>%
summarise(sum_point = sum(movie_point))
movie.review %>%
group_by(title) %>%
summarise(movie_sum = sum(movie_point))
movie.review %>%
group_by(movie_point) %>%
summarise(movie_sum = sum(movie_point))
library(ggplot2)
# 영화 리뷰의 내용만 선택하여 최빈 단어 10개를 뽑아 워드클라우드
library(KoNLP)
library(stringr)
library(ggplot2)
library(wordcloud)
movie <- movie.review
str(movie)
movie$movie_point <- as.numeric(movie$movie_point)
str(movie)
movie %>%
group_by(title) %>%
summarise(point_mean = mean(movie_point))
movie %>%
group_by(title) %>%
summarise(point_mean = mean(movie_point)) %>%
head(10)
movie %>%
group_by(title) %>%
summarise(point_mean = mean(movie_point)) %>%
head(10) %>%
arrange(desc(point_mean))
movie %>%
group_by(title) %>%
summarise(point_mean = mean(movie_point),
movie_cnt = n()) %>%
head(10) %>%
arrange(desc(point_mean))
movie %>%
group_by(title) %>%
summarise(point_mean = mean(movie_point),
movie_cnt = n()) %>%
head(10) %>%
arrange(desc(point_mean), desc(movie_cnt))
result <- movie %>%
group_by(title) %>%
summarise(point.mean = mean(movie_point),
point.sum = sum(movie_point),
n = n()) %>%
arrange(desc(point.mean), desc(point.sum)) %>%
head(10)
result
? data.frame
# (2)
head(USArrests)
str(USArrests)
library(ggiraphExtra)
library(mapproj)
library(maps)
library(tibble)
install.packages(c("isoband", "stringi"))
install.packages(c("isoband", "stringi"))
library(ggiraphExtra)
library(mapproj)
library(maps)
library(tibble)
library(ggplot2)
library(ggmap)
library(stringr)
crime <- rownames_to_column(USArrests, var = 'state')
crime
head(crime, 3)
map_data("state")
crime$state <- tolower(crime$state)
head(crime, 3)
# (3) 주별 위도, 경도 가져오기
state_map <- map_data('state')
view(state_map)
# (4) 지도 시각화
ggChoropleth(data = crime, # 지도에 표현할 데이터
aes(fill = Murder, # 지도에 채워질 변수
map_id = state), # 지도에 나타날 지역
map = state_map, # 위도, 경도 데이터
interactive = T)
devtools::install_github('cardiomoon/kormaps2014')
library(kormaps2014)
head(kormap1)
view(kormap1)
view(korpop1)
view(kormap1)
# https://cran.r-project.org/bin/windows/Rtools/ 에서 Rtools 설치하면 에러 수정된다.
library(kormaps2014)
view(kormap1)
view(korpop1)
library(ggmap)
library(stringr)
library(ggiraphExtra)
library(mapproj)
library(maps)
library(tibble)
library(ggplot2) # map_data('state')를 사용하면 미국 주별 위도,경도 데이터를 가져올 수 있다.
# https://cran.r-project.org/bin/windows/Rtools/ 에서 Rtools 설치하면 에러 수정된다.
library(kormaps2014)
View(kormap1)
View(korpop1)
# korpop2 2015 - 시군구별 인구 데이터
# korpop3 2015 - 읍면동별 인구 데이터
head(korpop1)
library(stringi)
# korpop1$name의 인코딩 바꾸기 (글자 깨져있음)
library(stringi)
# korpop1$name의 인코딩 바꾸기 (글자 깨져있음)
library(stringi)
# korpop1$name의 인코딩 바꾸기 (글자 깨져있음)
library(stringi)
# korpop1$name의 인코딩 바꾸기 (글자 깨져있음)
library(stringi)
# korpop1$name의 인코딩 바꾸기 (글자 깨져있음)
library(stringi)
head(korpop1[, c('행정구역별_읍면동', '총인구_명', 'code')])
library(dplyr)
korpop1 <- rename(korpop1, pop = 총인구_명,
name = 행정구역별_읍면동)
head(korpop1[, c('name', 'pop', 'code')])
korpop1$code
install.packages(c("blob", "broom", "cli", "credentials", "haven", "isoband", "jpeg", "matrixStats", "pillar", "RcppArmadillo", "readr", "rmarkdown", "rvest", "stringi", "tau", "tibble", "tinytex", "utf8", "vioplot", "xfun"))
install.packages("cli")
tapply(iris[, 'Sepal.Length'], iris['Species'], FUN = mean)
# 스프레이의 종류에 따른 살충효과를 점검해보기
datasets::InsectSprays
head(InsectSprays)
str(InsectSprays)
install.packages(c("gert", "insight", "survival"))
